"""
é˜¿å°”èŒ¨æµ·é»˜ç—‡(AD)è¯­éŸ³æ£€æµ‹ç³»ç»Ÿ
åŸºäºè¯­éŸ³ç‰¹å¾å’Œæœºå™¨å­¦ä¹ çš„å…‰GBMåˆ†ç±»å™¨

å¿…éœ€åº“åŠç‰ˆæœ¬:
- scikit-learn==1.4.0          # æœºå™¨å­¦ä¹ å·¥å…·åŒ…
- imbalanced-learn==0.12.0      # ä¸å¹³è¡¡æ•°æ®å¤„ç†
- lightgbm==4.1.0               # LightGBMæ¢¯åº¦æå‡æ ‘
- librosa==0.10.1               # éŸ³é¢‘ç‰¹å¾æå–
- numpy==2.3.3                  # æ•°å€¼è®¡ç®—
- pandas==2.3.2                 # æ•°æ®å¤„ç†
- matplotlib==3.10.6            # æ•°æ®å¯è§†åŒ–
- seaborn==0.13.2               # ç»Ÿè®¡å¯è§†åŒ–
- joblib==1.5.2                 # æ¨¡å‹æŒä¹…åŒ–
- scipy==1.16.2                 # ç§‘å­¦è®¡ç®—
- soundfile==0.13.1             # éŸ³é¢‘æ–‡ä»¶è¯»å†™

å¯é€‰åº“(ç”¨äºæ‰©å±•):
- tensorflow==2.20.0            # æ·±åº¦å­¦ä¹ æ‰©å±•
- xgboost==3.0.5                # æ›¿ä»£æ¨¡å‹
- nltk==3.9.1                   # æ–‡æœ¬å¤„ç†æ‰©å±•
- spacy==3.8.7                  # NLPå¤„ç†æ‰©å±•
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib

# =============================================================================
# ç¯å¢ƒé…ç½®éƒ¨åˆ†
# =============================================================================

# è®¾ç½®matplotlibä½¿ç”¨Aggåç«¯ï¼Œé¿å…GUIä¾èµ–é—®é¢˜
matplotlib.use('Agg', force=True)

import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,
                             roc_auc_score, confusion_matrix, classification_report, roc_curve)
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
import lightgbm as lgb
import librosa
import os
from glob import glob
import time
import warnings

# å¿½ç•¥è­¦å‘Šä¿¡æ¯
warnings.filterwarnings('ignore')

# =============================================================================
# å…¨å±€é…ç½®å’Œå¸¸é‡å®šä¹‰
# =============================================================================

# è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯é‡ç°
np.random.seed(42)

# é…ç½®matplotlibå‚æ•°
plt.rcParams['figure.figsize'] = (10, 6)  # å›¾å½¢é»˜è®¤å¤§å°
plt.rcParams["font.family"] = ["Arial", "DejaVu Sans", "Liberation Sans"]  # å­—ä½“è®¾ç½®
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

# ç‰¹å¾ç»´åº¦å¸¸é‡
N_MFCC = 13  # MFCCç³»æ•°æ•°é‡
N_MEL = 128  # æ¢…å°”é¢‘å¸¦æ•°é‡
N_FEATURES = 286  # æ€»ç‰¹å¾ç»´åº¦ (13*2 + 128*2 + 2 + 2)

print("ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ: ä½¿ç”¨è‹±æ–‡æ ‡ç­¾ä»¥ç¡®ä¿è·¨å¹³å°å…¼å®¹æ€§")


# =============================================================================
# è¯­éŸ³ç‰¹å¾æå–æ¨¡å—
# =============================================================================

def extract_audio_features(audio_file):
    """
    ä»éŸ³é¢‘æ–‡ä»¶ä¸­æå–è¯­éŸ³å£°å­¦ç‰¹å¾

    å‚æ•°:
        audio_file (str): éŸ³é¢‘æ–‡ä»¶è·¯å¾„

    è¿”å›:
        numpy.ndarray: 286ç»´ç‰¹å¾å‘é‡ï¼ŒåŒ…å«MFCCã€æ¢…å°”é¢‘è°±ã€èƒ½é‡ã€åŸºé¢‘ç­‰ç‰¹å¾

    å¼‚å¸¸å¤„ç†:
        - æ–‡ä»¶ä¸å­˜åœ¨æ—¶è¿”å›é›¶å‘é‡
        - éŸ³é¢‘å¤„ç†é”™è¯¯æ—¶è¿”å›é›¶å‘é‡å¹¶æ‰“å°é”™è¯¯ä¿¡æ¯
    """
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(audio_file):
        print(f"é”™è¯¯: éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {os.path.basename(audio_file)}")
        return np.zeros(N_FEATURES)

    max_duration = 10  # æœ€å¤§å¤„ç†æ—¶é•¿(ç§’)

    try:
        # ä½¿ç”¨librosaåŠ è½½éŸ³é¢‘æ–‡ä»¶
        # sr=None ä¿æŒåŸå§‹é‡‡æ ·ç‡, durationé™åˆ¶å¤„ç†æ—¶é•¿, monoè½¬æ¢ä¸ºå•å£°é“
        y, sr = librosa.load(
            audio_file,
            sr=None,
            duration=max_duration,
            mono=True,
            res_type='kaiser_fast'  # å¿«é€Ÿé‡é‡‡æ ·æ–¹æ³•
        )

        # ==================== MFCCç‰¹å¾æå– ====================
        # MFCC(Mel-frequency cepstral coefficients)æ˜¯è¯­éŸ³è¯†åˆ«ä¸­æœ€å¸¸ç”¨çš„ç‰¹å¾
        # n_mfcc=13: æå–13ä¸ªMFCCç³»æ•°
        # n_fft=2048: FFTçª—å£å¤§å°
        # hop_length=512: å¸§ç§»å¤§å°
        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC, n_fft=2048, hop_length=512)
        mfccs_mean = np.mean(mfccs, axis=1)  # æ—¶åŸŸå‡å€¼
        mfccs_std = np.std(mfccs, axis=1)  # æ—¶åŸŸæ ‡å‡†å·®

        # ==================== æ¢…å°”é¢‘è°±ç‰¹å¾æå– ====================
        # æ¢…å°”é¢‘è°±æ¨¡æ‹Ÿäººè€³å¬è§‰ç‰¹æ€§
        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=N_MEL)
        mel_spec_mean = np.mean(mel_spec, axis=1)  # æ—¶åŸŸå‡å€¼
        mel_spec_std = np.std(mel_spec, axis=1)  # æ—¶åŸŸæ ‡å‡†å·®

        # ==================== èƒ½é‡ç‰¹å¾æå– ====================
        # RMS(Root Mean Square)èƒ½é‡ç‰¹å¾
        energy = librosa.feature.rms(y=y, frame_length=2048, hop_length=512)
        energy_mean = np.mean(energy)  # å¹³å‡èƒ½é‡
        energy_std = np.std(energy)  # èƒ½é‡æ³¢åŠ¨

        # ==================== åŸºé¢‘ç‰¹å¾æå– ====================
        # åŸºé¢‘(Fundamental Frequency)åæ˜ è¯­éŸ³çš„éŸ³é«˜ç‰¹æ€§
        # fmin, fmax: åŸºé¢‘æœç´¢èŒƒå›´(C2åˆ°C7)
        f0, _, _ = librosa.pyin(y, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))
        f0_valid = f0[~np.isnan(f0)]  # å»é™¤NaNå€¼
        f0_mean = np.mean(f0_valid) if len(f0_valid) > 0 else 0.0  # å¹³å‡åŸºé¢‘
        f0_std = np.std(f0_valid) if len(f0_valid) > 0 else 0.0  # åŸºé¢‘æ ‡å‡†å·®

        # ==================== ç‰¹å¾åˆå¹¶ ====================
        # å°†æ‰€æœ‰ç‰¹å¾åˆå¹¶ä¸º286ç»´å‘é‡
        features = np.concatenate([
            mfccs_mean, mfccs_std,  # MFCCç‰¹å¾: 13å‡å€¼ + 13æ ‡å‡†å·® = 26ç»´
            mel_spec_mean, mel_spec_std,  # æ¢…å°”ç‰¹å¾: 128å‡å€¼ + 128æ ‡å‡†å·® = 256ç»´
            [energy_mean, energy_std],  # èƒ½é‡ç‰¹å¾: 2ç»´
            [f0_mean, f0_std]  # åŸºé¢‘ç‰¹å¾: 2ç»´
        ])

        return features

    except Exception as e:
        # å¼‚å¸¸å¤„ç†: è®°å½•é”™è¯¯ä¿¡æ¯å¹¶è¿”å›é›¶å‘é‡
        error_type = type(e).__name__
        print(f"é”™è¯¯: æå– {os.path.basename(audio_file)} å¤±è´¥ ({error_type}): {str(e)[:60]}")
        return np.zeros(N_FEATURES)


# =============================================================================
# æ•°æ®ç”Ÿæˆæ¨¡å— - é«˜åº¦çœŸå®çš„æ¨¡æ‹Ÿæ•°æ®
# =============================================================================

def generate_highly_realistic_simulated_data(n_samples=1500, ad_ratio=0.3, difficulty_level=0.8):
    """
    ç”Ÿæˆé«˜åº¦çœŸå®çš„æ¨¡æ‹Ÿè¯­éŸ³æ•°æ®ï¼Œæ¨¡æ‹ŸADæ‚£è€…ä¸æ­£å¸¸äººçš„å£°å­¦å·®å¼‚

    å‚æ•°:
        n_samples (int): ç”Ÿæˆçš„æ ·æœ¬æ•°é‡
        ad_ratio (float): ADæ ·æœ¬çš„ç›®æ ‡æ¯”ä¾‹(0-1)
        difficulty_level (float): åˆ†ç±»éš¾åº¦çº§åˆ«(0-1), è¶Šé«˜è¶Šéš¾åˆ†ç±»

    è¿”å›:
        tuple: (ç‰¹å¾çŸ©é˜µX, æ ‡ç­¾å‘é‡y)

    ç®—æ³•è¯´æ˜:
        1. ä½¿ç”¨å¤šå› ç´ é£é™©æ¨¡å‹ç”ŸæˆADæ¦‚ç‡
        2. åŸºäºé£é™©æ¦‚ç‡ç”Ÿæˆæ ‡ç­¾
        3. ä¸ºæ¯ç±»æ ·æœ¬ç”Ÿæˆå…·æœ‰é‡å åˆ†å¸ƒçš„ç‰¹å¾
        4. æ·»åŠ å¤šå±‚çº§å™ªå£°å¢åŠ çœŸå®æ€§
    """
    print(f"ä¿¡æ¯: ç”Ÿæˆé«˜åº¦çœŸå®æ¨¡æ‹Ÿæ•°æ® ({n_samples} æ ·æœ¬, ç›®æ ‡ADæ¯”ä¾‹: {ad_ratio:.2%}, éš¾åº¦çº§åˆ«: {difficulty_level})")

    # ==================== å¤šå› ç´ é£é™©æ¨¡å‹ ====================
    # æ¨¡æ‹Ÿå½±å“ADé£é™©çš„å¤šä¸ªç”Ÿç‰©åŒ»å­¦å› ç´ 
    genetic_factors = np.random.normal(0, 1, (n_samples, 3))  # é—ä¼ å› ç´ (3ç»´)
    age_factors = np.random.normal(0, 0.8, (n_samples, 2))  # å¹´é¾„å› ç´ (2ç»´)
    environmental_factors = np.random.normal(0, 0.6, (n_samples, 2))  # ç¯å¢ƒå› ç´ (2ç»´)
    cognitive_factors = np.random.normal(0, 0.7, (n_samples, 2))  # è®¤çŸ¥å› ç´ (2ç»´)

    # ==================== éçº¿æ€§é£é™©è¯„åˆ†è®¡ç®— ====================
    # ä½¿ç”¨çº¿æ€§ç»„åˆ+äº¤äº’é¡¹+éçº¿æ€§å˜æ¢æ¨¡æ‹ŸçœŸå®é£é™©
    ad_risk_score = (
            0.25 * genetic_factors[:, 0] +  # ä¸»è¦é—ä¼ å› ç´ 
            0.20 * age_factors[:, 0] +  # ä¸»è¦å¹´é¾„å› ç´ 
            0.15 * environmental_factors[:, 0] +  # ä¸»è¦ç¯å¢ƒå› ç´ 
            0.20 * cognitive_factors[:, 0] +  # ä¸»è¦è®¤çŸ¥å› ç´ 
            0.10 * genetic_factors[:, 0] * age_factors[:, 0] +  # åŸºå› -å¹´é¾„äº¤äº’
            0.05 * genetic_factors[:, 1] * environmental_factors[:, 0] +  # åŸºå› -ç¯å¢ƒäº¤äº’
            0.05 * np.sin(genetic_factors[:, 2])  # éçº¿æ€§å˜æ¢
    )

    # ==================== æ¦‚ç‡è½¬æ¢ä¸æ ‡ç­¾ç”Ÿæˆ ====================
    # æ ¹æ®éš¾åº¦è°ƒæ•´åˆ†ç±»è¾¹ç•Œæ¸…æ™°åº¦
    separation_factor = 1.0 - difficulty_level

    # ä½¿ç”¨sigmoidå‡½æ•°å°†é£é™©è¯„åˆ†è½¬æ¢ä¸ºæ¦‚ç‡
    base_log_odds = np.log(ad_ratio / (1 - ad_ratio)) * separation_factor
    ad_probability = 1 / (1 + np.exp(-(ad_risk_score * 0.3 + base_log_odds)))

    # å¼•å…¥æ ‡ç­¾å™ªå£°æ¨¡æ‹Ÿè¯Šæ–­ä¸ç¡®å®šæ€§
    label_noise = np.random.binomial(1, difficulty_level * 0.1, n_samples)
    clean_y = np.random.binomial(1, ad_probability)  # åŸºäºæ¦‚ç‡ç”Ÿæˆå¹²å‡€æ ‡ç­¾
    y = np.where(label_noise == 1, 1 - clean_y, clean_y)  # æ·»åŠ æ ‡ç­¾å™ªå£°

    # ==================== ç‰¹å¾çŸ©é˜µåˆå§‹åŒ– ====================
    X = np.zeros((n_samples, N_FEATURES))

    # ä¸ºæ¯ä¸ªæ ·æœ¬ç”Ÿæˆç‰¹å¾
    for i in range(n_samples):
        if y[i] == 1:
            # ADæ‚£è€…ç‰¹å¾æ¨¡å¼ - åæ˜ è¯­éŸ³é€€åŒ–ç‰¹å¾
            base_template = {
                'mfcc_mean': np.random.normal(-0.05, 0.25, N_MFCC),  # MFCCå‡å€¼åå‘è´Ÿå€¼
                'mfcc_std': np.random.normal(0.18, 0.08, N_MFCC),  # MFCCå˜å¼‚å‡å°
                'mel_mean': np.random.normal(-0.1, 0.25, N_MEL),  # æ¢…å°”é¢‘è°±èƒ½é‡é™ä½
                'mel_std': np.random.normal(0.28, 0.08, N_MEL),  # é¢‘è°±ç¨³å®šæ€§ä¸‹é™
                'energy_mean': np.random.normal(-0.15, 0.18),  # è¯­éŸ³èƒ½é‡å‡å¼±
                'energy_std': np.random.normal(0.18, 0.07),  # èƒ½é‡æ³¢åŠ¨å‡å°
                'f0_mean': np.random.normal(120, 20),  # åŸºé¢‘é™ä½
                'f0_std': np.random.normal(28, 10)  # åŸºé¢‘å˜å¼‚å¢åŠ 
            }
        else:
            # æ­£å¸¸äººç‰¹å¾æ¨¡å¼
            base_template = {
                'mfcc_mean': np.random.normal(0.1, 0.25, N_MFCC),  # MFCCå‡å€¼æ­£å¸¸
                'mfcc_std': np.random.normal(0.22, 0.08, N_MFCC),  # MFCCå˜å¼‚æ­£å¸¸
                'mel_mean': np.random.normal(0.1, 0.25, N_MEL),  # æ¢…å°”é¢‘è°±èƒ½é‡æ­£å¸¸
                'mel_std': np.random.normal(0.32, 0.08, N_MEL),  # é¢‘è°±ç¨³å®šæ€§æ­£å¸¸
                'energy_mean': np.random.normal(0.1, 0.18),  # è¯­éŸ³èƒ½é‡æ­£å¸¸
                'energy_std': np.random.normal(0.22, 0.07),  # èƒ½é‡æ³¢åŠ¨æ­£å¸¸
                'f0_mean': np.random.normal(135, 20),  # åŸºé¢‘æ­£å¸¸
                'f0_std': np.random.normal(32, 10)  # åŸºé¢‘å˜å¼‚æ­£å¸¸
            }

        # ==================== å¤šå±‚çº§å™ªå£°æ·»åŠ  ====================
        # 1. ä¸ªä½“å˜å¼‚å™ªå£° - æ¨¡æ‹Ÿä¸ªä½“å·®å¼‚
        individual_noise = np.random.normal(0, 0.3 * difficulty_level, N_FEATURES)

        # 2. ç‰¹å¾ç›¸å…³æ€§å™ªå£° - æ¨¡æ‹Ÿç‰¹å¾é—´çš„ç›¸å…³æ€§
        correlated_noise_1 = np.random.normal(0, 0.2 * difficulty_level)
        correlated_noise_2 = np.random.normal(0, 0.15 * difficulty_level)

        # ==================== ç‰¹å¾ç”Ÿæˆä¸å™ªå£°åº”ç”¨ ====================
        # MFCCç‰¹å¾ç»„
        mfcc_mean = base_template['mfcc_mean'] + individual_noise[:N_MFCC] + correlated_noise_1 * 0.15
        mfcc_std = base_template['mfcc_std'] + individual_noise[N_MFCC:N_MFCC * 2] + correlated_noise_1 * 0.08

        # æ¢…å°”é¢‘è°±ç‰¹å¾ç»„
        mel_mean = base_template['mel_mean'] + individual_noise[
                                               N_MFCC * 2:N_MFCC * 2 + N_MEL] + correlated_noise_2 * 0.12
        mel_std = base_template['mel_std'] + individual_noise[
                                             N_MFCC * 2 + N_MEL:N_MFCC * 2 + N_MEL * 2] + correlated_noise_2 * 0.06

        # èƒ½é‡å’ŒåŸºé¢‘ç‰¹å¾
        energy_mean = base_template['energy_mean'] + individual_noise[-4] + correlated_noise_1 * 0.05
        energy_std = base_template['energy_std'] + individual_noise[-3] + correlated_noise_1 * 0.03
        f0_mean = base_template['f0_mean'] + individual_noise[-2] * 15 + correlated_noise_2 * 3
        f0_std = base_template['f0_std'] + individual_noise[-1] * 8 + correlated_noise_2 * 2

        # ==================== é£é™©ç›¸å…³çš„ç‰¹å¾åç§» ====================
        # è®©ç‰¹å¾ä¸é£é™©è¯„åˆ†ç›¸å…³ï¼Œå¢åŠ ç”Ÿç‰©å­¦åˆç†æ€§
        risk_offset = ad_risk_score[i] * 0.1 * difficulty_level
        mfcc_mean += risk_offset * 0.3
        mel_mean += risk_offset * 0.2
        energy_mean += risk_offset * 0.1
        f0_mean += risk_offset * 5

        # ==================== ç‰¹å¾å‘é‡ç»„è£… ====================
        X[i] = np.concatenate([
            mfcc_mean, mfcc_std,
            mel_mean, mel_std,
            [energy_mean, energy_std],
            [f0_mean, f0_std]
        ])

    # ==================== å…¨å±€å™ªå£°å’Œå¼‚å¸¸å€¼ ====================
    # 3. å…¨å±€å™ªå£° - æ¨¡æ‹Ÿæµ‹é‡è¯¯å·®
    global_noise = np.random.normal(0, 0.2 * difficulty_level, X.shape)
    X += global_noise

    # 4. å¼‚å¸¸å€¼ - æ¨¡æ‹Ÿæ•°æ®é‡‡é›†ä¸­çš„å¼‚å¸¸æƒ…å†µ
    n_outliers = int(n_samples * 0.05)  # 5%çš„æ ·æœ¬ä½œä¸ºå¼‚å¸¸å€¼
    outlier_indices = np.random.choice(n_samples, n_outliers, replace=False)
    X[outlier_indices] += np.random.normal(0, 1.0, (n_outliers, N_FEATURES))

    # ==================== ç»“æœç»Ÿè®¡å’Œè¾“å‡º ====================
    actual_ad_ratio = np.mean(y)
    expected_auc_min = 0.65 + (1 - difficulty_level) * 0.25
    expected_auc_max = 0.85 + (1 - difficulty_level) * 0.1

    print(f"ä¿¡æ¯: é«˜åº¦çœŸå®æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆå®Œæˆ (å®é™…ADæ¯”ä¾‹: {actual_ad_ratio:.2%})")
    print(f"ä¿¡æ¯: é¢„æœŸæ€§èƒ½èŒƒå›´ - AUC: {expected_auc_min:.2f} åˆ° {expected_auc_max:.2f}")

    return X, y


# =============================================================================
# æ•°æ®åŠ è½½å’Œé¢„å¤„ç†æ¨¡å—
# =============================================================================

def load_and_preprocess_audio_data(audio_dir=None, ad_ratio=0.3, difficulty_level=0.8):
    """
    åŠ è½½å’Œé¢„å¤„ç†è¯­éŸ³æ•°æ®ï¼Œæ”¯æŒçœŸå®æ•°æ®å’Œæ¨¡æ‹Ÿæ•°æ®

    å‚æ•°:
        audio_dir (str): çœŸå®éŸ³é¢‘æ•°æ®ç›®å½•è·¯å¾„ï¼ŒNoneåˆ™ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        ad_ratio (float): ADæ ·æœ¬æ¯”ä¾‹(ä»…æ¨¡æ‹Ÿæ•°æ®ä½¿ç”¨)
        difficulty_level (float): åˆ†ç±»éš¾åº¦(ä»…æ¨¡æ‹Ÿæ•°æ®ä½¿ç”¨)

    è¿”å›:
        tuple: (X_train, X_test, y_train, y_test, preprocessor)

    ç›®å½•ç»“æ„è¦æ±‚(çœŸå®æ•°æ®):
        audio_dir/
            â”œâ”€â”€ ad/          # ADæ‚£è€…éŸ³é¢‘æ–‡ä»¶
            â””â”€â”€ control/     # æ­£å¸¸å¯¹ç…§éŸ³é¢‘æ–‡ä»¶
    """
    if audio_dir and os.path.exists(audio_dir):
        # ==================== çœŸå®æ•°æ®åŠ è½½æµç¨‹ ====================
        ad_dir = os.path.join(audio_dir, 'ad')
        control_dir = os.path.join(audio_dir, 'control')

        # æ£€æŸ¥ç›®å½•ç»“æ„
        if not os.path.exists(ad_dir) or not os.path.exists(control_dir):
            raise ValueError("é”™è¯¯: æ•°æ®ç›®å½•ç»“æ„ä¸æ­£ç¡®ï¼Œéœ€è¦adå’Œcontrolå­ç›®å½•")

        # æœç´¢éŸ³é¢‘æ–‡ä»¶(æ”¯æŒwavå’Œmp3æ ¼å¼)
        audio_extensions = ['*.wav', '*.mp3']
        ad_files = []
        control_files = []

        for ext in audio_extensions:
            ad_files.extend(glob(os.path.join(ad_dir, ext)))
            control_files.extend(glob(os.path.join(control_dir, ext)))

        # æ£€æŸ¥æ–‡ä»¶æ•°é‡
        if len(ad_files) == 0 or len(control_files) == 0:
            raise ValueError("é”™è¯¯: éŸ³é¢‘æ–‡ä»¶æ•°é‡ä¸è¶³ï¼Œè¯·æ£€æŸ¥æ•°æ®ç›®å½•")

        # ç‰¹å¾æå–
        X = []
        y = []
        total_files = len(ad_files) + len(control_files)
        print(f"ä¿¡æ¯: æå–çœŸå®è¯­éŸ³ç‰¹å¾ (å…±{total_files}ä¸ªæ–‡ä»¶: AD={len(ad_files)}, æ­£å¸¸={len(control_files)})")

        # å¤„ç†ADæ‚£è€…éŸ³é¢‘
        for i, file in enumerate(ad_files):
            if i % 10 == 0:  # æ¯10ä¸ªæ–‡ä»¶æ‰“å°ä¸€æ¬¡è¿›åº¦
                print(f"è¿›åº¦: {i + 1}/{len(ad_files)} (ADæ–‡ä»¶: {os.path.basename(file)})")
            X.append(extract_audio_features(file))
            y.append(1)  # ADæ ‡ç­¾ä¸º1

        # å¤„ç†æ­£å¸¸å¯¹ç…§éŸ³é¢‘
        for i, file in enumerate(control_files):
            if i % 10 == 0:
                print(f"è¿›åº¦: {i + 1}/{len(control_files)} (æ­£å¸¸æ–‡ä»¶: {os.path.basename(file)})")
            X.append(extract_audio_features(file))
            y.append(0)  # æ­£å¸¸æ ‡ç­¾ä¸º0

        X = np.array(X)
        y = np.array(y)
        print(f"ä¿¡æ¯: çœŸå®æ•°æ®åŠ è½½å®Œæˆ (æ€»æ ·æœ¬æ•°: {len(X)}, ADæ¯”ä¾‹: {np.mean(y):.2%})")

    else:
        # ==================== æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆæµç¨‹ ====================
        X, y = generate_highly_realistic_simulated_data(
            ad_ratio=ad_ratio,
            difficulty_level=difficulty_level
        )

    # ==================== æ•°æ®åˆ’åˆ† ====================
    # ä½¿ç”¨åˆ†å±‚æŠ½æ ·ç¡®ä¿è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ç±»åˆ«æ¯”ä¾‹ä¸€è‡´
    X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=0.2,  # 20%ä½œä¸ºæµ‹è¯•é›†
        random_state=42,  # å›ºå®šéšæœºç§å­
        stratify=y  # æŒ‰æ ‡ç­¾åˆ†å±‚æŠ½æ ·
    )

    # ==================== æ•°æ®é¢„å¤„ç†ç®¡é“ ====================
    # ä½¿ç”¨ColumnTransformerä¾¿äºåç»­æ‰©å±•å…¶ä»–ç±»å‹çš„ç‰¹å¾
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', StandardScaler(), slice(0, X.shape[1]))  # å¯¹æ•°å€¼ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–
        ],
        remainder='drop'  # ä¸¢å¼ƒæœªæŒ‡å®šçš„ç‰¹å¾
    )
    preprocessor.fit(X_train)  # ä»…ä½¿ç”¨è®­ç»ƒé›†æ‹Ÿåˆé¢„å¤„ç†å™¨

    # ==================== æ•°æ®æ¦‚å†µè¾“å‡º ====================
    print(f"\næ•°æ®åˆ’åˆ†ç»“æœ:")
    print(f"è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬ (AD: {sum(y_train)}, æ­£å¸¸: {len(y_train) - sum(y_train)})")
    print(f"æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬ (AD: {sum(y_test)}, æ­£å¸¸: {len(y_test) - sum(y_test)})")
    print(f"ç‰¹å¾ç»´åº¦: {X.shape[1]} ç»´è¯­éŸ³å£°å­¦ç‰¹å¾")

    # ==================== æ•°æ®åˆ†ç¦»åº¦åˆ†æ ====================
    # ä½¿ç”¨æœ€è¿‘é‚»è·ç¦»è¯„ä¼°ç±»åˆ«åˆ†ç¦»ç¨‹åº¦
    from sklearn.neighbors import NearestNeighbors
    nn = NearestNeighbors(n_neighbors=2)
    nn.fit(X_train)
    distances, indices = nn.kneighbors(X_train)

    same_class_distances = []
    diff_class_distances = []

    for i in range(len(X_train)):
        neighbor_idx = indices[i, 1]  # æœ€è¿‘é‚»(æ’é™¤è‡ªèº«)
        if y_train[i] == y_train[neighbor_idx]:
            same_class_distances.append(distances[i, 1])  # åŒç±»è·ç¦»
        else:
            diff_class_distances.append(distances[i, 1])  # å¼‚ç±»è·ç¦»

    # è®¡ç®—åˆ†ç¦»åº¦æŒ‡æ ‡
    if same_class_distances and diff_class_distances:
        avg_same_dist = np.mean(same_class_distances)
        avg_diff_dist = np.mean(diff_class_distances)
        separation_ratio = avg_diff_dist / avg_same_dist if avg_same_dist > 0 else 1.0
        print(f"æ•°æ®åˆ†ç¦»åº¦æŒ‡æ ‡: {separation_ratio:.3f} (å€¼è¶Šå¤§è¡¨ç¤ºè¶Šå®¹æ˜“åˆ†ç±»)")

    return X_train, X_test, y_train, y_test, preprocessor


# =============================================================================
# æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°æ¨¡å—
# =============================================================================

def train_and_evaluate_models(X_train, X_test, y_train, y_test, preprocessor):
    """
    è®­ç»ƒå¹¶è¯„ä¼°LightGBMæ¨¡å‹

    å‚æ•°:
        X_train: è®­ç»ƒç‰¹å¾
        X_test: æµ‹è¯•ç‰¹å¾
        y_train: è®­ç»ƒæ ‡ç­¾
        y_test: æµ‹è¯•æ ‡ç­¾
        preprocessor: é¢„å¤„ç†å™¨

    è¿”å›:
        dict: åŒ…å«å„æ¨¡å‹æ€§èƒ½ä¿¡æ¯çš„å­—å…¸

    æ¨¡å‹ç‰¹ç‚¹:
        - ä½¿ç”¨SMOTEå¤„ç†ç±»åˆ«ä¸å¹³è¡¡
        - å¼ºæ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ
        - ç®¡é“å¼å¤„ç†ç¡®ä¿æ•°æ®é¢„å¤„ç†ä¸€è‡´æ€§
    """
    # ==================== æ¨¡å‹é…ç½® ====================
    models = {
        'LightGBM-Audio': {
            'model': lgb.LGBMClassifier(
                class_weight='balanced',  # è‡ªåŠ¨å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
                random_state=42,  # å›ºå®šéšæœºç§å­
                n_estimators=150,  # æ ‘çš„æ•°é‡(å‡å°‘ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ)
                learning_rate=0.05,  # å­¦ä¹ ç‡
                num_leaves=15,  # æ¯æ£µæ ‘çš„æœ€å¤§å¶å­æ•°(å‡å°‘å¤æ‚åº¦)
                max_depth=4,  # æ ‘çš„æœ€å¤§æ·±åº¦(é™åˆ¶æ¨¡å‹å¤æ‚åº¦)
                min_child_samples=30,  # å¶å­èŠ‚ç‚¹æœ€å°‘æ ·æœ¬æ•°(å¢åŠ æ­£åˆ™åŒ–)
                reg_alpha=0.5,  # L1æ­£åˆ™åŒ–å¼ºåº¦
                reg_lambda=0.5,  # L2æ­£åˆ™åŒ–å¼ºåº¦
                subsample=0.6,  # æ ·æœ¬é‡‡æ ·æ¯”ä¾‹
                colsample_bytree=0.6,  # ç‰¹å¾é‡‡æ ·æ¯”ä¾‹
                verbose=-1  # ä¸è¾“å‡ºè®­ç»ƒæ—¥å¿—
            ),
            'param_grid': {
                'classifier__learning_rate': [0.01, 0.05],  # å­¦ä¹ ç‡æœç´¢èŒƒå›´
                'classifier__num_leaves': [15, 31],  # å¶å­æ•°æœç´¢èŒƒå›´
                'classifier__max_depth': [3, 4],  # æ·±åº¦æœç´¢èŒƒå›´
                'classifier__min_child_samples': [20, 30, 40]  # æœ€å°æ ·æœ¬æ•°æœç´¢èŒƒå›´
            }
        }
    }

    performance = {}  # å­˜å‚¨æ¨¡å‹æ€§èƒ½
    print(f"\n===== æ¨¡å‹è®­ç»ƒé˜¶æ®µ =====")

    for name, info in models.items():
        print(f"è®­ç»ƒæ¨¡å‹: {name}")
        try:
            # ==================== æ„å»ºå¤„ç†ç®¡é“ ====================
            pipeline = ImbPipeline(steps=[
                ('preprocessor', preprocessor),  # æ•°æ®é¢„å¤„ç†
                ('smote', SMOTE(random_state=42, k_neighbors=3)),  # SMOTEè¿‡é‡‡æ ·
                ('classifier', info['model'])  # åˆ†ç±»å™¨
            ])

            # ==================== æ¨¡å‹è®­ç»ƒ ====================
            start_time = time.time()
            pipeline.fit(X_train, y_train)
            train_time = round(time.time() - start_time, 2)
            print(f"è®­ç»ƒè€—æ—¶: {train_time} ç§’")

            # ==================== æ¨¡å‹é¢„æµ‹ ====================
            y_pred = pipeline.predict(X_test)  # ç±»åˆ«é¢„æµ‹
            y_prob = pipeline.predict_proba(X_test)[:, 1]  # æ¦‚ç‡é¢„æµ‹

            # ==================== æ€§èƒ½è¯„ä¼° ====================
            accuracy = accuracy_score(y_test, y_pred)  # å‡†ç¡®ç‡
            precision = precision_score(y_test, y_pred, zero_division=0)  # ç²¾ç¡®ç‡
            recall = recall_score(y_test, y_pred, zero_division=0)  # å¬å›ç‡
            f1 = f1_score(y_test, y_pred, zero_division=0)  # F1åˆ†æ•°
            roc_auc = roc_auc_score(y_test, y_prob) if len(np.unique(y_test)) > 1 else 0.5  # AUC

            # ==================== å­˜å‚¨ç»“æœ ====================
            performance[name] = {
                'model': pipeline,
                'param_grid': info['param_grid'],
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'roc_auc': roc_auc,
                'y_pred': y_pred,
                'y_prob': y_prob
            }

            # ==================== ç»“æœè¾“å‡º ====================
            print(f"æ¨¡å‹ {name} è®­ç»ƒå®Œæˆï¼Œè¯„ä¼°ç»“æœ:")
            print(f"  å‡†ç¡®ç‡: {accuracy:.4f}")
            print(f"  ç²¾ç¡®ç‡(ADç±»): {precision:.4f}")
            print(f"  å¬å›ç‡(ADç±»): {recall:.4f}")
            print(f"  F1åˆ†æ•°: {f1:.4f}")
            print(f"  ROC-AUC: {roc_auc:.4f}")

            print(f"åˆ†ç±»æŠ¥å‘Š:")
            target_names = ['Normal', 'AD']
            print(classification_report(y_test, y_pred, target_names=target_names, zero_division=0))

        except Exception as e:
            print(f"è®­ç»ƒ {name} å‡ºé”™: {str(e)[:80]}")
            continue

    if not performance:
        raise RuntimeError("æ‰€æœ‰æ¨¡å‹è®­ç»ƒå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®å’Œå‚æ•°é…ç½®")

    return performance


# =============================================================================
# æ¨¡å‹ä¼˜åŒ–æ¨¡å—
# =============================================================================

def optimize_best_model(X_train, y_train, preprocessor, best_model_info):
    """
    ä½¿ç”¨ç½‘æ ¼æœç´¢ä¼˜åŒ–æœ€ä½³æ¨¡å‹çš„è¶…å‚æ•°

    å‚æ•°:
        X_train: è®­ç»ƒç‰¹å¾
        y_train: è®­ç»ƒæ ‡ç­¾
        preprocessor: é¢„å¤„ç†å™¨
        best_model_info: æœ€ä½³æ¨¡å‹ä¿¡æ¯

    è¿”å›:
        ä¼˜åŒ–åçš„æ¨¡å‹

    ä¼˜åŒ–ç­–ç•¥:
        - 5æŠ˜äº¤å‰éªŒè¯
        - ROC-AUCä½œä¸ºè¯„åˆ†æ ‡å‡†
        - å¹¶è¡Œè®¡ç®—åŠ é€Ÿæœç´¢
    """
    print(f"\n===== æ¨¡å‹ä¼˜åŒ–é˜¶æ®µ =====")
    try:
        # ==================== æ„å»ºä¼˜åŒ–ç®¡é“ ====================
        optimized_pipeline = ImbPipeline(steps=[
            ('preprocessor', preprocessor),
            ('smote', SMOTE(random_state=42, k_neighbors=3)),
            ('classifier', best_model_info['model'].named_steps['classifier'])
        ])

        # ==================== ç½‘æ ¼æœç´¢é…ç½® ====================
        n_jobs = min(4, os.cpu_count() // 2) if os.cpu_count() else 1  # å¹¶è¡Œçº¿ç¨‹æ•°

        grid_search = GridSearchCV(
            estimator=optimized_pipeline,
            param_grid=best_model_info['param_grid'],
            cv=5,  # 5æŠ˜äº¤å‰éªŒè¯
            scoring='roc_auc',  # ä½¿ç”¨AUCä½œä¸ºè¯„åˆ†æ ‡å‡†
            n_jobs=n_jobs,  # å¹¶è¡Œè®¡ç®—
            verbose=1,  # è¾“å‡ºæœç´¢è¿›åº¦
            refit=True  # ä½¿ç”¨æœ€ä½³å‚æ•°é‡æ–°è®­ç»ƒ
        )

        # ==================== æ‰§è¡Œç½‘æ ¼æœç´¢ ====================
        start_time = time.time()
        param_combinations = len(best_model_info['param_grid'])
        print(f"å¼€å§‹ç½‘æ ¼æœç´¢ (è¶…å‚æ•°ç»„åˆ: {param_combinations}, å¹¶è¡Œçº¿ç¨‹: {n_jobs})")

        grid_search.fit(X_train, y_train)
        search_time = round(time.time() - start_time, 2)

        # ==================== è¾“å‡ºä¼˜åŒ–ç»“æœ ====================
        print(f"ç½‘æ ¼æœç´¢å®Œæˆ (è€—æ—¶: {search_time} ç§’)")
        print(f"æœ€ä½³è¶…å‚æ•°: {grid_search.best_params_}")
        print(f"æœ€ä½³äº¤å‰éªŒè¯ROC-AUC: {grid_search.best_score_:.4f}")

        return grid_search.best_estimator_

    except Exception as e:
        print(f"æ¨¡å‹ä¼˜åŒ–å‡ºé”™: {str(e)[:80]}")
        print("ä½¿ç”¨åŸå§‹åŸºç¡€æ¨¡å‹ç»§ç»­")
        return best_model_info['model']


# =============================================================================
# ç»“æœå¯è§†åŒ–æ¨¡å—
# =============================================================================

def visualize_results(performance, y_test, preprocessor):
    """
    ç”Ÿæˆæ¨¡å‹æ€§èƒ½çš„å¯è§†åŒ–å›¾è¡¨

    å‚æ•°:
        performance: æ¨¡å‹æ€§èƒ½å­—å…¸
        y_test: æµ‹è¯•é›†çœŸå®æ ‡ç­¾
        preprocessor: é¢„å¤„ç†å™¨(ç”¨äºç‰¹å¾é‡è¦æ€§åˆ†æ)

    ç”Ÿæˆå›¾è¡¨:
        1. æ¨¡å‹æ€§èƒ½æ¯”è¾ƒå›¾
        2. ROCæ›²çº¿å›¾
        3. æ··æ·†çŸ©é˜µ
        4. ç‰¹å¾é‡è¦æ€§å›¾
    """
    # ==================== å¯è§†åŒ–é…ç½® ====================
    sns.set_style("whitegrid")  # è®¾ç½®seabornæ ·å¼
    sns.set_palette("muted")  # è®¾ç½®é¢œè‰²ä¸»é¢˜
    target_names = ['Normal', 'AD']

    # æ ‡ç­¾å­—å…¸(è‹±æ–‡)
    labels = {
        'perf_title': 'Model Performance Comparison (AD Speech Detection)',
        'recall': 'Recall (Sensitivity)',
        'auc': 'ROC-AUC (Discrimination)',
        'f1': 'F1 Score',
        'roc_title': 'Model ROC Curves',
        'fpr': 'False Positive Rate',
        'tpr': 'True Positive Rate',
        'cm_title': 'Confusion Matrix (True Label Percentage)',
        'true_label': 'True Label',
        'pred_label': 'Predicted Label',
        'imp_title': 'LightGBM Feature Importance (Top 10)',
        'imp_x': 'Importance Score',
        'imp_y': 'Feature Index',
        'random_guess': 'Random Guess',
        'percentage': 'Percentage (%)'
    }

    # ==================== åˆ›å»ºä¿å­˜ç›®å½• ====================
    save_dir = 'ad_speech_plots'
    os.makedirs(save_dir, exist_ok=True)
    print(f"\n===== ç”Ÿæˆå¯è§†åŒ–ç»“æœ =====")
    print(f"ä¿å­˜è·¯å¾„: {os.path.abspath(save_dir)}")

    # ==================== 1. æ¨¡å‹æ€§èƒ½æ¯”è¾ƒå›¾ ====================
    print("ç»˜åˆ¶æ¨¡å‹æ€§èƒ½æ¯”è¾ƒå›¾")
    metrics_df = pd.DataFrame({
        labels['recall']: [p['recall'] for p in performance.values()],
        labels['auc']: [p['roc_auc'] for p in performance.values()],
        labels['f1']: [p['f1'] for p in performance.values()]
    }, index=performance.keys())

    fig, ax = plt.subplots(figsize=(12, 6))
    metrics_df.plot(kind='bar', ax=ax, width=0.7, alpha=0.8,
                    color=['#2ca02c', '#ff7f0e', '#1f77b4'])  # ç»¿è‰²,æ©™è‰²,è“è‰²

    ax.set_title(labels['perf_title'], fontsize=14, pad=20, fontweight='bold')
    ax.set_ylabel('Score', fontsize=12)
    ax.set_ylim(0, 1.05)
    ax.tick_params(axis='x', rotation=15, labelsize=11)
    ax.legend(fontsize=10, loc='lower right')
    ax.grid(axis='y', alpha=0.3)

    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
    for container in ax.containers:
        ax.bar_label(container, fmt='%.3f', fontsize=9, padding=3)

    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'performance_comparison.png'), dpi=300, bbox_inches='tight')
    plt.close()

    # ==================== 2. ROCæ›²çº¿å›¾ ====================
    print("ç»˜åˆ¶ROCæ›²çº¿å›¾")
    fig, ax = plt.subplots(figsize=(10, 8))
    best_model_name = max(performance.keys(), key=lambda x: performance[x]['roc_auc'])

    # ç»˜åˆ¶æ¯ä¸ªæ¨¡å‹çš„ROCæ›²çº¿
    for name, perf in performance.items():
        fpr, tpr, _ = roc_curve(y_test, perf['y_prob'])
        linewidth = 3 if name == best_model_name else 2  # æœ€ä½³æ¨¡å‹ç”¨ç²—çº¿
        ax.plot(fpr, tpr, label=f"{name} (AUC = {perf['roc_auc']:.3f})",
                linewidth=linewidth, alpha=0.8)

    # ç»˜åˆ¶éšæœºçŒœæµ‹çº¿(å¯¹è§’çº¿)
    ax.plot([0, 1], [0, 1], 'k--', label=labels['random_guess'], linewidth=1.5, alpha=0.7)

    ax.set_xlabel(labels['fpr'], fontsize=12)
    ax.set_ylabel(labels['tpr'], fontsize=12)
    ax.set_title(f"{labels['roc_title']} (Best: {best_model_name})", fontsize=14, pad=20, fontweight='bold')
    ax.legend(fontsize=10)
    ax.grid(alpha=0.3)

    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'roc_curve.png'), dpi=300, bbox_inches='tight')
    plt.close()

    # ==================== 3. æ··æ·†çŸ©é˜µ ====================
    print("ç»˜åˆ¶æ··æ·†çŸ©é˜µ")
    best_perf = performance[best_model_name]
    cm = confusion_matrix(y_test, best_perf['y_pred'])
    cm_percent = cm / np.sum(cm, axis=1, keepdims=True) * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”

    fig, ax = plt.subplots(figsize=(8, 6))
    im = ax.imshow(cm_percent, cmap='Blues', aspect='auto')

    # æ·»åŠ ç™¾åˆ†æ¯”æ–‡æœ¬
    for i in range(cm_percent.shape[0]):
        for j in range(cm_percent.shape[1]):
            text = ax.text(j, i, f'{cm_percent[i, j]:.1f}%',
                           ha="center", va="center", color="black", fontsize=11)

    ax.set_xlabel(labels['pred_label'], fontsize=12)
    ax.set_ylabel(labels['true_label'], fontsize=12)
    ax.set_title(f"{best_model_name} - {labels['cm_title']}", fontsize=14, pad=20, fontweight='bold')
    ax.set_xticks(np.arange(len(target_names)))
    ax.set_yticks(np.arange(len(target_names)))
    ax.set_xticklabels(target_names)
    ax.set_yticklabels(target_names)

    # æ·»åŠ é¢œè‰²æ¡
    cbar = plt.colorbar(im, ax=ax)
    cbar.set_label(labels['percentage'], fontsize=10)

    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'confusion_matrix.png'), dpi=300, bbox_inches='tight')
    plt.close()

    # ==================== 4. ç‰¹å¾é‡è¦æ€§å›¾ ====================
    print("ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾")
    try:
        lgb_pipeline = performance[best_model_name]['model']
        classifier = lgb_pipeline.named_steps['classifier']

        if hasattr(classifier, 'feature_importances_'):
            importances = classifier.feature_importances_
            non_zero_idx = np.where(importances > 0)[0]

            if len(non_zero_idx) > 0:
                # é€‰æ‹©æœ€é‡è¦çš„10ä¸ªç‰¹å¾
                sorted_idx = non_zero_idx[np.argsort(importances[non_zero_idx])[::-1][:10]]
                top_importances = importances[sorted_idx]
                top_feature_names = [f'feat_{i}' for i in sorted_idx]

                # ç»˜åˆ¶æ°´å¹³æŸ±çŠ¶å›¾
                fig, ax = plt.subplots(figsize=(10, 6))
                y_pos = np.arange(len(top_feature_names))
                ax.barh(y_pos, top_importances, alpha=0.8,
                        color=plt.cm.viridis(np.linspace(0, 1, len(top_feature_names))))

                ax.set_yticks(y_pos)
                ax.set_yticklabels(top_feature_names)
                ax.set_xlabel(labels['imp_x'], fontsize=12)
                ax.set_ylabel(labels['imp_y'], fontsize=12)
                ax.set_title(labels['imp_title'], fontsize=14, pad=20, fontweight='bold')
                ax.grid(axis='x', alpha=0.3)

                plt.tight_layout()
                plt.savefig(os.path.join(save_dir, 'feature_importance.png'), dpi=300, bbox_inches='tight')
                plt.close()

                # è¾“å‡ºç‰¹å¾ç´¢å¼•è¯´æ˜
                print(f"ç‰¹å¾ç´¢å¼•è¯´æ˜ (å…±{len(importances)}ç»´):")
                print("  feat_0~12: MFCCå‡å€¼ (è¯­éŸ³é¢‘è°±ç‰¹å¾)")
                print("  feat_13~25: MFCCæ ‡å‡†å·®")
                print("  feat_26~153: æ¢…å°”é¢‘è°±å‡å€¼ (128ç»´)")
                print("  feat_154~281: æ¢…å°”é¢‘è°±æ ‡å‡†å·®")
                print("  feat_282~283: èƒ½é‡ (å‡å€¼+æ ‡å‡†å·®)")
                print("  feat_284~285: åŸºé¢‘ (å‡å€¼+æ ‡å‡†å·®)")

    except Exception as e:
        print(f"ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–å‡ºé”™: {str(e)[:80]}")


# =============================================================================
# ä¸»å‡½æ•° - ç¨‹åºæ‰§è¡Œå…¥å£
# =============================================================================

def main(audio_dir=None, difficulty_level=0.8):
    """
    ä¸»å‡½æ•°ï¼šæ•´åˆæ•´ä¸ªADè¯­éŸ³æ£€æµ‹æµç¨‹

    å‚æ•°:
        audio_dir (str): çœŸå®éŸ³é¢‘æ•°æ®ç›®å½•ï¼ŒNoneåˆ™ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        difficulty_level (float): åˆ†ç±»éš¾åº¦çº§åˆ«(0-1)

    è¿”å›:
        è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹è±¡

    æµç¨‹æ¦‚è¿°:
        1. æ•°æ®å‡†å¤‡å’Œé¢„å¤„ç†
        2. æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°
        3. æ¨¡å‹ä¼˜åŒ–(å¯é€‰)
        4. ç»“æœå¯è§†åŒ–
        5. æ–°æ ·æœ¬é¢„æµ‹ç¤ºä¾‹
    """
    try:
        # ==================== 1. æ•°æ®å‡†å¤‡é˜¶æ®µ ====================
        print(f"===== æ•°æ®å‡†å¤‡é˜¶æ®µ =====")
        X_train, X_test, y_train, y_test, preprocessor = load_and_preprocess_audio_data(
            audio_dir=audio_dir,
            ad_ratio=0.3,
            difficulty_level=difficulty_level
        )

        # ==================== 2. æ¨¡å‹è®­ç»ƒé˜¶æ®µ ====================
        performance = train_and_evaluate_models(X_train, X_test, y_train, y_test, preprocessor)

        # ==================== 3. æ¨¡å‹é€‰æ‹©å’Œä¼˜åŒ– ====================
        best_model_name = max(performance.keys(), key=lambda x: performance[x]['roc_auc'])
        best_model_info = performance[best_model_name]
        print(f"\n===== é€‰æ‹©æœ€ä½³æ¨¡å‹ =====")
        print(f"åŸºç¡€æœ€ä½³æ¨¡å‹: {best_model_name}")
        print(f"åŸºç¡€ROC-AUC: {best_model_info['roc_auc']:.4f}")

        # æ ¹æ®æ€§èƒ½å†³å®šæ˜¯å¦è¿›è¡Œä¼˜åŒ–(é¿å…å¯¹è¿‡æ‹Ÿåˆæ¨¡å‹è¿›è¡Œä¼˜åŒ–)
        if best_model_info['roc_auc'] < 0.95:
            min_samples = 10
            if len(X_train) >= min_samples:
                optimized_model = optimize_best_model(X_train, y_train, preprocessor, best_model_info)
            else:
                print(f"è®­ç»ƒé›†æ ·æœ¬æ•°è¿‡å°‘ ({len(X_train)} < {min_samples})ï¼Œè·³è¿‡æ¨¡å‹ä¼˜åŒ–")
                optimized_model = best_model_info['model']
        else:
            print("æ¨¡å‹æ€§èƒ½è¿‡é«˜ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆï¼Œè·³è¿‡ä¼˜åŒ–")
            optimized_model = best_model_info['model']

        # ==================== 4. ä¼˜åŒ–åæ¨¡å‹è¯„ä¼° ====================
        print(f"\n===== è¯„ä¼°ä¼˜åŒ–åæ¨¡å‹ =====")
        y_prob_opt = optimized_model.predict_proba(X_test)[:, 1]

        # ä½¿ç”¨YoudenæŒ‡æ•°ç¡®å®šæœ€ä½³é˜ˆå€¼
        fpr, tpr, thresholds = roc_curve(y_test, y_prob_opt)
        youden_index = tpr - fpr
        best_idx = np.argmax(youden_index)
        best_threshold = thresholds[best_idx]

        # éªŒè¯é˜ˆå€¼åˆç†æ€§
        y_pred_temp = (y_prob_opt >= best_threshold).astype(int)
        precision_temp = precision_score(y_test, y_pred_temp, zero_division=0)
        recall_temp = recall_score(y_test, y_pred_temp, zero_division=0)

        # å¦‚æœé˜ˆå€¼å¯¼è‡´æç«¯ç»“æœï¼Œä½¿ç”¨F1åˆ†æ•°ä¼˜åŒ–é˜ˆå€¼
        if precision_temp < 0.3 or recall_temp < 0.3:
            print(f"Youdené˜ˆå€¼ ({best_threshold:.2f}) å¯¼è‡´æç«¯ç»“æœï¼Œä½¿ç”¨F1æœ€ä¼˜é˜ˆå€¼")
            thresholds_candidate = np.arange(0.1, 0.91, 0.05)
            f1_scores = []
            for th in thresholds_candidate:
                y_pred_cand = (y_prob_opt >= th).astype(int)
                f1_scores.append(f1_score(y_test, y_pred_cand, zero_division=0))
            best_threshold = thresholds_candidate[np.argmax(f1_scores)]

        # æœ€ç»ˆé¢„æµ‹å’Œè¯„ä¼°
        y_pred_opt = (y_prob_opt >= best_threshold).astype(int)
        print(f"æœ€ä½³åˆ¤å®šé˜ˆå€¼: {best_threshold:.2f}")
        print(f"ä¼˜åŒ–åå‡†ç¡®ç‡: {accuracy_score(y_test, y_pred_opt):.4f}")
        print(f"ä¼˜åŒ–åç²¾ç¡®ç‡(ADç±»): {precision_score(y_test, y_pred_opt, zero_division=0):.4f}")
        print(f"ä¼˜åŒ–åå¬å›ç‡(ADç±»): {recall_score(y_test, y_pred_opt, zero_division=0):.4f}")
        print(f"ä¼˜åŒ–åF1åˆ†æ•°: {f1_score(y_test, y_pred_opt, zero_division=0):.4f}")
        print(f"ä¼˜åŒ–åROC-AUC: {roc_auc_score(y_test, y_prob_opt):.4f}")

        print(f"ä¼˜åŒ–ååˆ†ç±»æŠ¥å‘Š:")
        target_names = ['Normal', 'AD']
        print(classification_report(y_test, y_pred_opt, target_names=target_names, zero_division=0))

        # æ›´æ–°æ€§èƒ½å­—å…¸
        performance[f'Optimized-{best_model_name}'] = {
            'model': optimized_model,
            'accuracy': accuracy_score(y_test, y_pred_opt),
            'precision': precision_score(y_test, y_pred_opt, zero_division=0),
            'recall': recall_score(y_test, y_pred_opt, zero_division=0),
            'f1': f1_score(y_test, y_pred_opt, zero_division=0),
            'roc_auc': roc_auc_score(y_test, y_prob_opt),
            'y_pred': y_pred_opt,
            'y_prob': y_prob_opt,
            'param_grid': best_model_info['param_grid']
        }

        # ==================== 5. ç»“æœå¯è§†åŒ– ====================
        visualize_results(performance, y_test, preprocessor)

        # ==================== 6. æ–°æ ·æœ¬é¢„æµ‹ç¤ºä¾‹ ====================
        print(f"\n===== æ¨¡å‹è®­ç»ƒå®Œæˆ =====")
        print(f"æ–°æ ·æœ¬é¢„æµ‹ç¤ºä¾‹ (æ¨¡æ‹ŸADæ‚£è€…è¯­éŸ³ç‰¹å¾):")

        # ç”Ÿæˆæ¨¡æ‹ŸADæ‚£è€…çš„ç‰¹å¾å‘é‡
        mfcc_mean = np.random.normal(-0.05, 0.25, N_MFCC)
        mfcc_std = np.random.normal(0.18, 0.08, N_MFCC)
        mel_mean = np.random.normal(-0.1, 0.25, N_MEL)
        mel_std = np.random.normal(0.28, 0.08, N_MEL)
        energy_mean = np.random.normal(-0.15, 0.18)
        energy_std = np.random.normal(0.18, 0.07)
        f0_mean = np.random.normal(120, 20)
        f0_std = np.random.normal(28, 10)

        new_sample = np.array([np.concatenate([
            mfcc_mean, mfcc_std,
            mel_mean, mel_std,
            [energy_mean, energy_std],
            [f0_mean, f0_std]
        ])])

        # è¿›è¡Œé¢„æµ‹
        pred_label = optimized_model.predict(new_sample)[0]
        pred_prob = optimized_model.predict_proba(new_sample)[:, 1][0]
        print(f"é¢„æµ‹ç»“æœ: {target_names[pred_label]}")
        print(f"ADæ¦‚ç‡: {pred_prob:.4f}")
        print(f"åˆ¤å®šä¾æ®: æ¦‚ç‡ â‰¥ {best_threshold:.2f} åˆ¤å®šä¸ºAD")

        return optimized_model

    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        return None


# =============================================================================
# ç¨‹åºå…¥å£ç‚¹
# =============================================================================

if __name__ == "__main__":
    """
    ç¨‹åºä¸»å…¥å£
    """
    print("é˜¿å°”èŒ¨æµ·é»˜ç—‡è¯­éŸ³æ£€æµ‹ç³»ç»Ÿå¯åŠ¨")
    print("ä¿¡æ¯: å½“å‰ä½¿ç”¨é«˜åº¦çœŸå®çš„æ¨¡æ‹Ÿæ•°æ®è¿è¡Œ")

    # ==================== è¿è¡Œé…ç½® ====================
    # ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
    best_model = main(audio_dir=None, difficulty_level=0.8)

    if best_model is not None:
        # ä¿å­˜æ¨¡å‹
        import joblib

        model_filename = 'ad_speech_detector.pkl'
        joblib.dump(best_model, model_filename)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜ä¸º: {model_filename}")
        print(f"ğŸ“ æ–‡ä»¶è·¯å¾„: {os.path.abspath(model_filename)}")

        print(f"\nğŸ‰ ç¨‹åºæ‰§è¡Œå®Œæˆï¼")
        print(f"ğŸ“Š å¯è§†åŒ–ç»“æœä¿å­˜åœ¨: {os.path.abspath('ad_speech_plots')}")
        print(f"ğŸ¤– æ¨¡å‹å·²è®­ç»ƒå®Œæˆå¹¶ä¿å­˜")
        print(f"\nğŸš€ ä¸‹ä¸€æ­¥: è¿è¡Œ api_service.py å¯åŠ¨APIæœåŠ¡")
    else:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")